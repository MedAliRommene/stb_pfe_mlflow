{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Lenovo\\\\Desktop\\\\stb_pfe_mlflow\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Lenovo\\\\Desktop\\\\stb_pfe_mlflow'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Lenovo\\\\Desktop\\\\stb_pfe_mlflow'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "train_data = pd.read_csv('artifacts/data_training/train.csv') \n",
    "test_data = pd.read_csv('artifacts/data_training/test.csv') # Remplacez 'stb_data.csv' par votre fichier de données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy (No PCA): 0.9669\n",
      "Random Forest Accuracy (No PCA): 0.9904\n",
      "Logistic Regression Accuracy (No PCA): 0.8548\n",
      "Gradient Boosting Accuracy (No PCA): 0.9932\n",
      "AdaBoost Accuracy (No PCA): 0.9895\n",
      "SVC Accuracy (No PCA): 0.8514\n",
      "Decision Tree Accuracy (No PCA): 0.9886\n",
      "Gaussian Naive Bayes Accuracy (No PCA): 0.7325\n",
      "Perceptron Accuracy (No PCA): 0.8292\n",
      "Extra Trees Accuracy (No PCA): 0.9904\n",
      "Logistic Regression CV Accuracy (No PCA): 0.8554\n",
      "Logistic Regression CV Cross-Validation Accuracy: 0.8449\n",
      "Logistic Regression Cross-Validation Accuracy: 0.8439\n",
      "SVC Cross-Validation Accuracy: 0.8430\n",
      "\n",
      "Final Accuracy Results Without PCA:\n",
      "KNN: 0.9669\n",
      "Random Forest: 0.9904\n",
      "Logistic Regression: 0.8548\n",
      "Gradient Boosting: 0.9932\n",
      "AdaBoost: 0.9895\n",
      "SVC: 0.8514\n",
      "Decision Tree: 0.9886\n",
      "Gaussian Naive Bayes: 0.7325\n",
      "Perceptron: 0.8292\n",
      "Extra Trees: 0.9904\n",
      "Logistic Regression CV: 0.8554\n",
      "The dataset has more than 2 features, scatter plot cannot be generated.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier, ExtraTreesClassifier)\n",
    "from sklearn.linear_model import (LogisticRegression, LogisticRegressionCV, Perceptron)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Remove 'tiers_key' and target 'Cluster' for X data\n",
    "train_x = train_data.drop(columns=[\"tiers_key\", \"Cluster\"])\n",
    "test_x = test_data.drop(columns=[\"tiers_key\", \"Cluster\"])\n",
    "\n",
    "# Extract the target\n",
    "train_y = train_data[\"Cluster\"]\n",
    "test_y = test_data[\"Cluster\"]\n",
    "\n",
    "# Encode all categorical features in the train and test datasets\n",
    "def encode_features(train_df, test_df):\n",
    "    # Combine train and test for consistent encoding\n",
    "    combined = pd.concat([train_df, test_df], axis=0)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for col in combined.select_dtypes(include=['object']).columns:\n",
    "        le = LabelEncoder()\n",
    "        combined[col] = le.fit_transform(combined[col].astype(str))\n",
    "    \n",
    "    # Split back into train and test sets\n",
    "    train_encoded = combined.iloc[:len(train_df)]\n",
    "    test_encoded = combined.iloc[len(train_df):]\n",
    "    \n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "train_x_encoded, test_x_encoded = encode_features(train_x, test_x)\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_y_encoded = label_encoder.fit_transform(train_y)\n",
    "test_y_encoded = label_encoder.transform(test_y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "train_x_scaled = scaler.fit_transform(train_x_encoded)\n",
    "test_x_scaled = scaler.transform(test_x_encoded)\n",
    "\n",
    "# Initialize multiple models for evaluation\n",
    "models = {\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5, weights=\"distance\"),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"SVC\": SVC(kernel='linear', probability=True),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"Perceptron\": Perceptron(max_iter=1000, random_state=42),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
    "    \"Logistic Regression CV\": LogisticRegressionCV(max_iter=200),\n",
    "}\n",
    "\n",
    "# Train and evaluate each model without PCA\n",
    "accuracy_results_no_pca = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(train_x_scaled, train_y_encoded)\n",
    "    predictions_no_pca = model.predict(test_x_scaled)\n",
    "    accuracy_no_pca = accuracy_score(test_y_encoded, predictions_no_pca)\n",
    "    accuracy_results_no_pca[model_name] = accuracy_no_pca\n",
    "    print(f\"{model_name} Accuracy (No PCA): {accuracy_no_pca:.4f}\")\n",
    "\n",
    "# Perform cross-validation for the top models\n",
    "top_models = {\n",
    "    \"Logistic Regression CV\": LogisticRegressionCV(max_iter=200),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
    "    \"SVC\": SVC(kernel='linear', probability=True),\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for model_name, model in top_models.items():\n",
    "    cv_scores = cross_val_score(model, train_x_scaled, train_y_encoded, cv=5, scoring='accuracy')\n",
    "    cv_results[model_name] = cv_scores.mean()\n",
    "    print(f\"{model_name} Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Display accuracy results without PCA\n",
    "print(\"\\nFinal Accuracy Results Without PCA:\")\n",
    "for model_name, accuracy in accuracy_results_no_pca.items():\n",
    "    print(f\"{model_name}: {accuracy:.4f}\")\n",
    "\n",
    "# Visualize the dataset with predicted labels\n",
    "# Assuming you're using SVC for the prediction and you have 2 features for visualization\n",
    "if train_x_scaled.shape[1] == 2:\n",
    "    svc_model = SVC(kernel='linear')\n",
    "    svc_model.fit(train_x_scaled, train_y_encoded)\n",
    "    pred = svc_model.predict(test_x_scaled)\n",
    "\n",
    "    # Plotting the data points\n",
    "    plt.scatter(test_x_scaled[:, 0], test_x_scaled[:, 1], c=pred, cmap='coolwarm')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('SVC Predictions on Test Data')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The dataset has more than 2 features, scatter plot cannot be generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize a dictionary to hold cross-validation results\n",
    "cv_results = {}\n",
    "\n",
    "# Evaluate all models with cross-validation\n",
    "for model_name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, train_x_scaled, train_y_encoded, cv=5, scoring='accuracy')\n",
    "    cv_results[model_name] = cv_scores.mean()\n",
    "    print(f\"{model_name} Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nCross-Validation Accuracy Results:\")\n",
    "for model_name, accuracy in cv_results.items():\n",
    "    print(f\"{model_name}: {accuracy:.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the cross-validation accuracy results you've provided, the Gradient Boosting model stands out as the best choice with a cross-validation accuracy of 0.9932. Here’s why:\n",
    "\n",
    "1. Highest Cross-Validation Accuracy\n",
    "Gradient Boosting has the highest cross-validation accuracy, indicating it performs better than all other models when generalizing to unseen data. This suggests it effectively captures the underlying patterns in the dataset.\n",
    "2. Consistency\n",
    "The high cross-validation score implies that Gradient Boosting is not likely overfitting. Models that perform well on training data but poorly on validation data might be overfitting, which can lead to poor performance in real-world applications. The high score across multiple folds of cross-validation indicates stability and robustness.\n",
    "3. Comparison with Other Models\n",
    "Other models, such as Extra Trees (0.9896) and Random Forest (0.9894), also exhibit high performance, but none reach the accuracy of Gradient Boosting. These models are also strong contenders, but their scores are slightly lower.\n",
    "The KNN and AdaBoost models also perform well (0.9628 and 0.9642), but again, they do not match the performance of Gradient Boosting.\n",
    "4. Importance in Practice\n",
    "In practical applications, achieving a model that can predict accurately across diverse conditions is crucial. Since Gradient Boosting excels in both cross-validation and accuracy, it’s likely to provide reliable predictions when deployed.\n",
    "Conclusion\n",
    "Therefore, the final decision is to choose the Gradient Boosting model. Its superior performance in terms of cross-validation accuracy makes it the most suitable choice for your dataset and modeling objectives. If interpretability is not a primary concern, this model will provide the best predictive performance. However, if you require a balance between performance and interpretability, you might also consider Random Forest or Extra Trees, which offer strong accuracy with relatively better interpretability than Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
